## Q1.py

Implemented Multilayer Perceptron with parameters: -
* **n layers**: Number of Layers (int)
* **layer sizes**: an array of size n layers which contains the number of nodes in each layer.
(array of int)
* **activation**: activation function to be used (string)
* **learning rate**: the learning rate to be used (float)
* **weight init**: initialization function to be used
  * **zero**: Zero initialization
  * **random**: Random initialization with a scaling factor of 0.01
  * **normal**: Normal(0,1) initialization with a scaling factor of 0.01
* **batch size**: batch size to be used (int)
* **num epochs**: number of epochs to be used (int)

## Q2.py

Used the MLP from Q1 on the MNIST dataset for various hyper-parameter settings

## Q3.py

Used PyTorch's MLP for a dataset.

## Q4.py

Used pre-trained Alexnet model from PyTorch.

